{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab32d33-f8ec-47d5-9801-4a6770760fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Install required libraries\n",
    "\n",
    "# Purpose: Install necessary Python libraries to interact with Google Cloud, process data, and develop machine learning models.\n",
    "# Libraries:\n",
    "# - google-cloud-aiplatform: For interacting with Google Cloud Vertex AI services.\n",
    "# - pandas: For data manipulation and analysis.\n",
    "# - scikit-learn: For machine learning model development and evaluation.\n",
    "\n",
    "!pip install google-cloud-aiplatform pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445b1fb-9180-48a0-a3ae-8848146d9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Initialize Vertex AI SDK\n",
    "\n",
    "# Purpose: Initialize the Vertex AI SDK to set up the connection to the Google Cloud platform using your project ID and region.\n",
    "# What it does: This command initializes the Google Cloud AI platform and configures your project ID and location for subsequent operations.\n",
    "# Outcome: You can now use Vertex AI services in the notebook to store and serve features.\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=\"your-project-id\", location=\"us-central1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e21c86-4ef3-48d5-9251-cce7cc3faaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Load dataset from Google Cloud Storage (GCS)\n",
    "\n",
    "# Purpose: Load the dataset stored in Google Cloud Storage (GCS) into a Pandas DataFrame to inspect and analyze it.\n",
    "# What it does: Reads a CSV file from GCS into a Pandas DataFrame and prints the first few rows of the dataset to understand its structure.\n",
    "# Outcome: The first few rows of the dataset are printed, allowing you to explore the structure of the data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"gs://<your-bucket-name>/user_activity.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c7750-989c-4652-b9b3-59df10049213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Create a new feature (average session duration)\n",
    "\n",
    "# Purpose: Create a new feature called 'avg_session_duration' to measure the average time spent per session for each user.\n",
    "# What it does: Calculates the average session duration by dividing 'time_spent' by 'session_count'.\n",
    "# Outcome: The dataset is updated with a new column 'avg_session_duration' containing the calculated values.\n",
    "\n",
    "df[\"avg_session_duration\"] = df[\"time_spent\"] / df[\"session_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0688b-67cc-4348-9351-871885d2edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Apply one-hot encoding on the 'activity_type' column\n",
    "# Purpose: Convert the categorical column 'activity_type' into binary (one-hot encoded) features for machine learning models.\n",
    "# What it does: Creates new binary columns for each unique value in 'activity_type' (e.g., browsing, cart, purchase, wishlist).\n",
    "# Outcome: The original 'activity_type' column is replaced by multiple binary columns representing each activity type.\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"activity_type\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24260a0-eb67-400d-a2ca-ec3815688c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Print the updated DataFrame with one-hot encoded columns\n",
    "\n",
    "print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62d7ef-9192-4125-817e-8d8e84ef4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Rename 'user_id' column to 'entity_id' for Vertex AI Feature Store\n",
    "# Purpose: Rename 'user_id' to 'entity_id' to meet the naming conventions of Vertex AI Feature Store (which requires 'entity_id' as the identifier).\n",
    "# What it does: Renames the 'user_id' column in the DataFrame to 'entity_id'.\n",
    "# Outcome: The DataFrame now contains the 'entity_id' column, which will be used as the primary key for feature store ingestion.\n",
    "\n",
    "df = df.rename(columns={\"user_id\": \"entity_id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03813b57-8ad9-45bc-8ee2-8d8335e9918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Convert 'entity_id' to string data type\n",
    "# Purpose: Convert the 'entity_id' column to a string data type, as required by Vertex AI Feature Store for entity identifiers.\n",
    "# What it does: Ensures that the 'entity_id' column is treated as a string by explicitly casting it to the 'str' type.\n",
    "# Outcome: The 'entity_id' column is now in string format, making it compatible with Vertex AI Feature Store.\n",
    "\n",
    "df[\"entity_id\"] = df[\"entity_id\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d0dd6-cdb4-46d6-8ba1-4f31226961f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9: Convert timestamp to datetime format: Ensure timestamp is in Native datetime64[ns] Format. Vertex AI Feature Store expects a strict TIMESTAMP type.\n",
    "# Purpose: Convert the 'timestamp' column to the correct datetime format for compatibility with Vertex AI Feature Store.\n",
    "# What it does: The 'timestamp' column is converted to a proper datetime format using Pandas' `to_datetime()` function.\n",
    "# Outcome: The 'timestamp' column will now be in the 'datetime64[ns]' format, ready for ingestion into Vertex AI Feature Store.\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35844a1e-1d09-4257-85e9-5db148b3fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10: Validate data types to ensure proper format\n",
    "# Purpose: Verify that the data types of the columns are correctly formatted.\n",
    "# What it does: Prints the data types of all columns in the DataFrame to ensure that the 'timestamp' and 'entity_id' columns have the correct formats.\n",
    "# Outcome: If any column's data type is incorrect, you can spot the error and fix it before ingestion.\n",
    "\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4dd624-4ed1-4b21-9638-64173aa4395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11: Drop rows with missing timestamps\n",
    "# Purpose: Clean the data by removing rows that contain invalid or missing timestamps.\n",
    "# What it does: Drops rows where the 'timestamp' column has missing or invalid values (i.e., NaT).\n",
    "# Outcome: The dataset is cleaned, and rows with invalid timestamps are removed to avoid issues when uploading data to the feature store.\n",
    "\n",
    "df = df.dropna(subset=[\"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45072d0d-fce6-4990-b658-61bc1054171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12: Create Vertex AI Feature Store with online serving capabilities\n",
    "# Purpose: Create a Vertex AI Feature Store to store and manage the engineered features for machine learning.\n",
    "# What it does: Creates a new feature store with online serving capabilities, which allows for fast retrieval of features for real-time predictions.\n",
    "# Outcome: A new feature store is created in the specified region, enabling you to store features for online serving.\n",
    "\n",
    "featurestore = aiplatform.Featurestore.create(\n",
    "    featurestore_id=\"user_features_store\",\n",
    "    location=\"us-central1\",\n",
    "    online_store_fixed_node_count=1  # Enables online serving with one node\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efee7c-54cb-44de-a28d-1b7ef532d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13: Create 'users' entity type in the feature store\n",
    "# Purpose: Define an entity type for users in the Vertex AI Feature Store to group user-specific features.\n",
    "# What it does: Creates an entity type named 'users' to logically organize user-related features (e.g., avg_session_duration, session_count).\n",
    "# Outcome: A new entity type 'users' is created within the feature store.\n",
    "\n",
    "entity_type = featurestore.create_entity_type(\n",
    "    entity_type_id=\"users\",\n",
    "    description=\"Features related to user activity\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90ebbb-e8b8-4383-bafd-66a13f4e55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14: Add features\n",
    "entity_type.create_feature(\n",
    "    feature_id=\"avg_session_duration\",\n",
    "    value_type=\"DOUBLE\",\n",
    "    description=\"Average session duration per user\"\n",
    ")\n",
    "\n",
    "entity_type.create_feature(\n",
    "    feature_id=\"session_count\",\n",
    "    value_type=\"INT64\",\n",
    "    description=\"Number of sessions per user\"\n",
    ")\n",
    "\n",
    "entity_type.create_feature(\n",
    "    feature_id=\"activity_type_browsing\",\n",
    "    value_type=\"INT64\",\n",
    "    description=\"Indicator for browsing activity\"\n",
    ")\n",
    "\n",
    "entity_type.create_feature(\n",
    "    feature_id=\"activity_type_cart\",\n",
    "    value_type=\"INT64\",\n",
    "    description=\"Indicator for cart activity\"\n",
    ")\n",
    "\n",
    "entity_type.create_feature(\n",
    "    feature_id=\"activity_type_purchase\",\n",
    "    value_type=\"INT64\",\n",
    "    description=\"Indicator for purchase activity\"\n",
    ")\n",
    "\n",
    "entity_type.create_feature(\n",
    "    feature_id=\"activity_type_wishlist\",\n",
    "    value_type=\"INT64\",\n",
    "    description=\"Indicator for wishlist activity\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42714b31-b12c-4cc5-815a-329950159238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15: Convert timestamp column to ISO 8601 format for Vertex AI compatibility\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Force Pandas datetime to match TIMESTAMP requirements\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Convert to ISO 8601 string format for TIMESTAMP compatibility\n",
    "df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "# Confirm the column's data type\n",
    "print(df[\"timestamp\"].head())\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595df6b9-09b5-41d4-b36b-b738a2f33ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16: Check and drop rows with invalid timestamps\n",
    "\n",
    "invalid_timestamps = df[df[\"timestamp\"].isna()]\n",
    "print(f\"Invalid rows: {len(invalid_timestamps)}\")\n",
    "\n",
    "# Drop rows with missing or invalid timestamps\n",
    "df = df.dropna(subset=[\"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dede6c9-fd2e-4605-9378-af79cd299128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17: Ensure 'timestamp' column is in native datetime format\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Validate the column type\n",
    "print(df[\"timestamp\"].dtypes)  # Should display 'datetime64[ns]'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129f37e-a443-4b89-9899-dfc26dbbf01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18: Remove rows with missing or invalid timestamp values: (If any)\n",
    "\n",
    "# Check for missing or invalid timestamps\n",
    "print(f\"Invalid timestamps: {df['timestamp'].isna().sum()}\")\n",
    "\n",
    "# Drop rows with invalid timestamps\n",
    "df = df.dropna(subset=[\"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ebf0c-5450-4de0-9b42-b13abf9b763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19: ingest (Upload) the feature-engineered data to the Feature Store.\n",
    "\n",
    "\n",
    "entity_type.ingest_from_df(\n",
    "    feature_ids=[\n",
    "        \"avg_session_duration\",\n",
    "        \"session_count\",\n",
    "        \"activity_type_browsing\",\n",
    "        \"activity_type_cart\",\n",
    "        \"activity_type_purchase\",\n",
    "        \"activity_type_wishlist\",\n",
    "    ],\n",
    "    feature_time=\"timestamp\",  # The column representing feature time\n",
    "    df_source=df,              # The processed DataFrame\n",
    "    entity_id_field=\"entity_id\"  # The column representing entity ID\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8561750-3442-42d7-83f3-14c9cf939f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Query the Feature Store to confirm the ingestion was successful\n",
    "\n",
    "features = entity_type.read(\n",
    "    entity_ids=[\"1\", \"2\"],  # Replace with actual entity IDs\n",
    "    feature_ids=[\n",
    "        \"avg_session_duration\",\n",
    "        \"session_count\",\n",
    "        \"activity_type_browsing\",\n",
    "        \"activity_type_cart\",\n",
    "        \"activity_type_purchase\",\n",
    "        \"activity_type_wishlist\",\n",
    "    ]\n",
    ")\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed5068-c130-4024-893d-e353e5b5e0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
