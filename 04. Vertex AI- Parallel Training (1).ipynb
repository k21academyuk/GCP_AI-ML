{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec51924-0c6e-48a4-afc4-5caec85ab1cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ebb1c0-05ce-4bd4-8001-b648703fae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-aiplatform\n",
    "# !pip install google-cloud-storage\n",
    "# !pip install kfp google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fba81e-d3cc-4414-ba8a-4f5254317d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger(\"logger\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics, Metrics, component, pipeline\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import Feature, Featurestore\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba90deee-531e-4d50-9a73-2aaf973b9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and define constants\n",
    "\n",
    "# Setup global variables\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# GCP project id \n",
    "PROJECT_ID = \"datacouch-vertexai\"\n",
    "\n",
    "# GCP Bucket name \n",
    "BUCKET_NAME = \"datacouch-vertex-ai\"\n",
    "GCS_BUCKET_NAME = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Bucket Folder\n",
    "BUCKET_FOLDER = \"forecasting-pipeline\"\n",
    "\n",
    "# Experiment Name\n",
    "EXPERIMENT_NAME = f\"{PROJECT_ID}\" \n",
    "\n",
    "# Pipeline\n",
    "PIPELINE_FOLDER = \"vertex_ai_demo\"\n",
    "PIPELINE_ROOT = f\"{GCS_BUCKET_NAME}/{BUCKET_FOLDER}/{PIPELINE_FOLDER}\"\n",
    "\n",
    "# Display Name\n",
    "DISPLAY_NAME = \"sku-forecast\"\n",
    "\n",
    "# Pipeline JSON Path\n",
    "PIPELINE_JSON_PKG_PATH = \"../pipelines/sku_forecast.json\"\n",
    "\n",
    "# Enter the region where you want to deploy the Vertex pipeline servoces\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfce011-8954-4c05-8380-85e3399d84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90f215-49c2-4b51-9e2e-960aa7dcb6a7",
   "metadata": {},
   "source": [
    "# Data Load Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb90a2a8-cf9e-46f0-8e3a-ce5ec0917203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from where these base images are picked and replace it with google artifact registry images\n",
    "\n",
    "# @component(base_image=\"python:3.9\", \n",
    "#            packages_to_install=[\"fsspec\", \"pandas\", \"gcsfs\"], \n",
    "#            output_component_file=\"../yaml_files/load_sku_data.yaml\")\n",
    "# def load_sku_data(url:str,\n",
    "#                   dataset: Output[Dataset], \n",
    "#                   kpi_output: Output[Metrics]) -> NamedTuple(\"output\", [(\"final_skus\", list)]):\n",
    "\n",
    "    \n",
    "#     # Step 1. Importing Packages\n",
    "#     from google.cloud import storage\n",
    "#     import pandas as pd\n",
    "    \n",
    "#     # Step 2. Load Data\n",
    "#     df = pd.read_csv(url, delimiter=\",\")[['material','calendar_year_month', 'qty_order_change']]\n",
    "    \n",
    "#     # Step 3. Data Pre-Processing\n",
    "#     df['calendar_year_month'] = pd.to_datetime(df['calendar_year_month'], format='%Y%m')\n",
    "    \n",
    "#     # Step 3.1. Save Interim Data\n",
    "#     # Step 4.1 Get List of SKU Ids\n",
    "#     sku_list = list(set(df.material.values.tolist()))\n",
    "    \n",
    "#     # Step 4.2 Save Different csv files for each SKUs and \n",
    "#     # Create a Dictionary of SKUs and it's corresponding data\n",
    "#     final_skus = []\n",
    "#     for sku_id in sku_list[:2]:\n",
    "#         path = dataset.path + f\"_{sku_id}.csv\"\n",
    "#         sku_df = df[df.material == sku_id][['calendar_year_month', 'qty_order_change']].copy()\n",
    "#         sku_df.to_csv(path , index=False, encoding='utf-8-sig')\n",
    "#         kpi_output.log_metric(f\"data_size_{sku_id}\", int(sku_df.shape[0]))\n",
    "#         final_skus.append({sku_id: path})\n",
    "    \n",
    "        \n",
    "#     return  (final_skus,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9698c7-69b4-4f7d-8d13-28c639388c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from where these base images are picked and replace it with google artifact registry images\n",
    "\n",
    "@component(base_image=\"python:3.9\", \n",
    "           packages_to_install=[\"fsspec\", \"pandas\", \"gcsfs\", \"pyarrow\", \"google-cloud-aiplatform\", \"google-cloud-bigquery-storage\"], \n",
    "           output_component_file=\"../yaml_files/load_sku_data.yaml\")\n",
    "def load_sku_data(featurestore_id: str,\n",
    "                  project:str,\n",
    "                  region:str,\n",
    "                  sku_list: list,\n",
    "                  dataset: Output[Dataset], \n",
    "                  kpi_output: Output[Metrics]) -> NamedTuple(\"output\", [(\"final_skus\", list)]):\n",
    "\n",
    "    \n",
    "    # Step 1. Importing Packages\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from google.cloud.aiplatform import Feature, Featurestore\n",
    "    \n",
    "    # Step 2. Retrieve Data from Feature Store\n",
    "    fs = Featurestore(\n",
    "    featurestore_name=featurestore_id,\n",
    "    project=project,\n",
    "    location=region\n",
    "    )\n",
    "    \n",
    "    SERVING_FEATURE_IDS = {\n",
    "    \"id\": ['*']\n",
    "    }\n",
    "    \n",
    "    final_skus = [] # to create a list of dictionary and corresponding csv files\n",
    "    for sku_id in sku_list:\n",
    "        READ_INSTANCES_URI = f'gs://datacouch-vertex-ai/forecasting-pipeline/read_instance/{sku_id}_input.csv' \n",
    "        path = dataset.path + f\"_{sku_id}_output.csv\"\n",
    "        df = pd.read_csv(READ_INSTANCES_URI)\n",
    "        df.timestamp = pd.to_datetime(df.timestamp)\n",
    "        output_df = fs.batch_serve_to_df(serving_feature_ids=SERVING_FEATURE_IDS, read_instances_df=df)\n",
    "        output_df[['calendar_year_month', 'qty_order_change']].to_csv(path , index=False, encoding='utf-8-sig')\n",
    "        kpi_output.log_metric(f\"data_size_{sku_id}\", int(output_df.shape[0]))\n",
    "        final_skus.append({sku_id: path})\n",
    "    \n",
    "        \n",
    "    return  (final_skus,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e6530-346f-46de-824b-44d33a2ed262",
   "metadata": {},
   "source": [
    "# Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db383ab2-a235-49e0-930c-59c9e3bf0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\"fsspec\", \"pandas\", \"scikit-learn\", \"gcsfs\", \"pystan\", \"prophet\"], \n",
    "    base_image=\"python:3.9\", \n",
    "    output_component_file=\"../yaml_files/train_model.yaml\"\n",
    ")\n",
    "def train_model(\n",
    "    sku_dict:dict,\n",
    "    bucket_name: str,\n",
    "    # destination_blob_name: str,\n",
    "    model: Output[Model], \n",
    "    kpi_output: Output[Metrics]) -> NamedTuple(\"output\", [(\"filename\", str)]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from prophet import Prophet\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # Step 2. Load Data\n",
    "    sku_id = list(sku_dict.keys())[0]\n",
    "    df = pd.read_csv(sku_dict[sku_id])\n",
    "    \n",
    "    # prepare expected column names\n",
    "    df.columns = ['ds', 'y']\n",
    "    df['ds']= pd.to_datetime(df['ds'])\n",
    "    data_size = df.shape[0]\n",
    "    \n",
    "    # define the model\n",
    "    train_model = Prophet()\n",
    "    \n",
    "    # fit the model\n",
    "    train_model.fit(df)\n",
    "    file_name = model.path + \".pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(train_model, file)\n",
    "\n",
    "    return (file_name,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf29c5-9ed6-4a8a-8c8a-3b78abf5e5c8",
   "metadata": {},
   "source": [
    "# Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6336b7-b287-4105-8769-3b0b796f67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"kfp\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"../deploy_model.yaml\"\n",
    ")\n",
    "def deploy_model(\n",
    "    sku_dict:dict,\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    serving_container_image_uri : str, \n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    ")-> NamedTuple(\n",
    "  'Outputs',\n",
    "  [\n",
    "    ('endpoint_resource_name', str)\n",
    "  ]):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    sku_id = list(sku_dict.keys())[0]\n",
    "    DISPLAY_NAME  = f\"{sku_id}_forecasting\"\n",
    "    MODEL_NAME = f\"{sku_id}_demand_forecasting\"\n",
    "    ENDPOINT_NAME = f\"{sku_id}_demand_forecasting\"\n",
    "    \n",
    "    def create_endpoint():\n",
    "        endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc',\n",
    "        project=project, \n",
    "        location=region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "            display_name = ENDPOINT_NAME, project=project, location=region\n",
    "        )\n",
    "        return endpoint\n",
    "    \n",
    "    endpoint = create_endpoint()\n",
    "    artifact_uri = str(model.uri.replace(f\"model\", \"\"))\n",
    "    t = str(endpoint)\n",
    "    test = t.split(\": \")[-1]\n",
    "\n",
    "\n",
    "    #Import a model programmatically\n",
    "    \n",
    "    models = aiplatform.Model.list(filter=(\"display_name={}\").format(DISPLAY_NAME))\n",
    "  \n",
    "    if len(models) == 0:\n",
    "      model_upload = aiplatform.Model.upload(\n",
    "        display_name = DISPLAY_NAME,\n",
    "        version_description=\"Linear Regression Model\",\n",
    "        version_aliases=[\"v1\"],\n",
    "        labels={\"release\": \"dev\"},\n",
    "        artifact_uri = model.uri[:-6], \n",
    "        serving_container_image_uri =  serving_container_image_uri,\n",
    "        serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "        serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "        serving_container_environment_variables={\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "        }, \n",
    "      )\n",
    "    else:\n",
    "        parent_model = models[0].resource_name\n",
    "        new_version_id = eval(models[0].version_id) + 1\n",
    "        model_upload = aiplatform.Model.upload(\n",
    "            display_name = DISPLAY_NAME, \n",
    "            artifact_uri = model.uri[:-6],\n",
    "            version_description=\"Linear Regression Model\",\n",
    "            version_aliases=[f\"v{new_version_id}\"],\n",
    "            labels={\"release\": \"dev\"},\n",
    "            serving_container_image_uri =  serving_container_image_uri,\n",
    "            serving_container_health_route=f\"/v{new_version_id}/models/{MODEL_NAME}\",\n",
    "            serving_container_predict_route=f\"/v{new_version_id}/models/{MODEL_NAME}:predict\",\n",
    "            serving_container_environment_variables={\n",
    "            \"MODEL_NAME\": MODEL_NAME,\n",
    "        },  \n",
    "            parent_model = parent_model\n",
    "        )\n",
    "    \n",
    "#     model_deploy = model_upload.deploy(\n",
    "#         machine_type=\"n1-standard-2\", \n",
    "#         endpoint=endpoint,\n",
    "#         traffic_split={\"0\":100},\n",
    "#         deployed_model_display_name=DISPLAY_NAME,\n",
    "#     )\n",
    "\n",
    "# #   Save data to the output params\n",
    "#     vertex_model.uri = model_deploy.resource_name\n",
    "    return (\"success\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568f53a-c2da-4cc5-8a59-a28cb9d6aeba",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c402a18-4927-4ea3-a396-467ef85f0cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e95f3b0-b840-4ead-89e2-7e7397547b5b",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aac67f9-1c58-4cce-baee-8765526f2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context\n",
    "    name=\"sku-forecast-pipeline-job\",\n",
    ")\n",
    "def pipeline(\n",
    "    url: str = \"gs://datacouch-vertex-ai/forecasting-pipeline/raw_data/sku_day_level.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "    serving_container_image_uri: str = \"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\" # \"asia-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\"\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    notify_train_task = VertexNotificationEmailOp(recipients=[\"shadab.cs0058@gmail.com\"])\n",
    "    with dsl.ExitHandler(notify_train_task, name=\"pipeline-status\"):\n",
    "        data_op = load_sku_data(featurestore_id=\"sku_forecasting\",\n",
    "                  project=project,\n",
    "                  region=region,\n",
    "                  sku_list=['8ghjhj', 'hjkjhiu']).set_retry(num_retries=2, backoff_duration='10s', \n",
    "                                               backoff_factor=2, backoff_max_duration ='60s')\n",
    "        with dsl.ParallelFor(data_op.outputs[\"final_skus\"], parallelism=2) as sku:\n",
    "            train_model_op = train_model(sku_dict=sku, bucket_name=BUCKET_NAME).after(data_op)\n",
    "            deploy_model_op = deploy_model(sku_dict=sku,\n",
    "            model=train_model_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            region=region, \n",
    "            serving_container_image_uri = serving_container_image_uri,\n",
    "            ).after(train_model_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30e6061-34ab-4309-a56b-9c4f477de8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path=PIPELINE_JSON_PKG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6d16c37-0389-4081-9ba4-f9368d3337ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = aiplatform.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=PIPELINE_JSON_PKG_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    project=PROJECT_ID,\n",
    "    enable_caching=True,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e94eb2-63bc-4308-9cdd-b9f53c3febcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.gcs_utils:Creating GCS bucket for Vertex Pipelines: \"datacouch-vertex-ai\"\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/808205789435/locations/us-central1/pipelineJobs/sku-forecast-pipeline-job-20250202144600\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/808205789435/locations/us-central1/pipelineJobs/sku-forecast-pipeline-job-20250202144600')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/sku-forecast-pipeline-job-20250202144600?project=808205789435\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8d107-c24c-4156-a58c-ea287e555b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
