{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1553-5582-43c0-b111-952967bed0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Summary of the imported libraries\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Purpose: The following libraries are imported to facilitate data manipulation, machine learning, and Google Cloud AI integration.\n",
    "# What it does: \n",
    "#    - 'pandas' provides data structures (like DataFrame) to manipulate, analyze, and preprocess large datasets.\n",
    "#    - 'aiplatform' enables interaction with Google Cloud's Vertex AI for managing datasets, training, and deploying machine learning models.\n",
    "#    - 'train_test_split' is used to split the dataset into training and test subsets for proper model evaluation.\n",
    "#    - 'RandomForestClassifier' is a machine learning algorithm from scikit-learn that builds an ensemble of decision trees to classify data.\n",
    "#    - 'accuracy_score', 'confusion_matrix', and 'classification_report' are evaluation metrics to assess the performance of the model, including accuracy, precision, recall, and F1-score.\n",
    "# Outcome: These libraries together provide all necessary tools to build, evaluate, and deploy machine learning models efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93fb2a-86f7-4db9-9b01-fede425ff3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Actions for Google Cloud Vertex AI integration\n",
    "\n",
    "aiplatform.init(project=\"<project-id>\", location=\"us-central1\")\n",
    "\n",
    "featurestore = aiplatform.Featurestore(\"<Feature Store Name>\")\n",
    "entity_type = featurestore.get_entity_type(\"users\")\n",
    "\n",
    "# Purpose: The following commands initialize Vertex AI, interact with a Feature Store, and retrieve an entity type.\n",
    "# What it does:\n",
    "#    - 'aiplatform.init' initializes the connection to Vertex AI using the specified project ID and location (in this case, 'us-central1').\n",
    "#    - 'Featurestore' creates an instance of the Vertex AI Feature Store, enabling storage and management of features for machine learning models.\n",
    "#    - 'get_entity_type' retrieves the entity type (e.g., 'users') from the Feature Store, which represents the structure of the data stored in the Feature Store.\n",
    "# Outcome: These commands set up the necessary environment and access for working with Vertex AI and retrieving feature data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc108e75-f187-4556-8af5-4d85b827a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Actions for fetching features from the Vertex AI Feature Store\n",
    "\n",
    "# Fetch features for a set of users\n",
    "features = entity_type.read(\n",
    "    entity_ids=[\"1\", \"2\", \"3\", \"4\"],  # Example entity IDs (user IDs)\n",
    "    feature_ids=[\"avg_session_duration\", \"session_count\", \"activity_type_browsing\", \"activity_type_cart\", \"activity_type_purchase\"]\n",
    ")\n",
    "\n",
    "# Convert the features to a pandas DataFrame\n",
    "df = pd.DataFrame(features)\n",
    "print(df.head())\n",
    "\n",
    "# Purpose: The following commands retrieve specific features for users from Vertex AI Feature Store and convert them into a DataFrame.\n",
    "# What it does:\n",
    "#    - 'entity_type.read' is used to fetch feature data for a set of entity IDs (in this case, user IDs). The 'feature_ids' parameter specifies which features to retrieve for each user.\n",
    "#    - 'pd.DataFrame(features)' converts the retrieved features into a Pandas DataFrame for easy manipulation and analysis.\n",
    "#    - 'print(df.head())' displays the first few rows of the DataFrame to provide an overview of the fetched data.\n",
    "# Outcome: This code fetches features from the Feature Store and presents them in a structured DataFrame format for further analysis or model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188195b-07dc-4dad-8e1b-d92c6ea45969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Actions for selecting input features (X) and target labels (Y)\n",
    "\n",
    "# Select input features (X) and target labels (Y)\n",
    "x = df[[\"avg_session_duration\", \"session_count\", \"activity_type_browsing\", \"activity_type_cart\", \"activity_type_purchase\"]]\n",
    "y = df[\"activity_type_browsing\"]  # We'll predict browsing activity. You can swap this for other target columns.\n",
    "\n",
    "# Purpose: The following commands define the input features (X) and target labels (Y) for training a machine learning model.\n",
    "# What it does:\n",
    "#    - 'x' selects the columns representing the features (independent variables) from the DataFrame, which will be used to predict the target label.\n",
    "#    - 'y' selects the target column (dependent variable), which in this case is the 'activity_type_browsing'. You can choose different columns for other prediction tasks.\n",
    "# Outcome: This code prepares the features and target labels to be used for training a model, with X representing input data and Y the predicted outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589e275-42d3-4d32-b19d-3658cb9b92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Actions for splitting the data into training and test sets\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Purpose: The following command splits the dataset into training and test sets for model evaluation.\n",
    "# What it does:\n",
    "#    - 'train_test_split' randomly splits the data into training and test subsets.\n",
    "#    - 'test_size=0.2' specifies that 20% of the data will be used for testing, and the remaining 80% will be used for training.\n",
    "#    - 'random_state=42' ensures reproducibility by fixing the random seed.\n",
    "# Outcome: This code prepares the data for model training and evaluation by splitting it into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b290809-59bd-473e-ad13-0613e7ec9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Actions for training the machine learning model\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Purpose: The following commands train a machine learning model using the RandomForestClassifier.\n",
    "# What it does:\n",
    "#    - 'RandomForestClassifier(n_estimators=100, random_state=42)' initializes a random forest classifier with 100 trees, and a fixed random seed for reproducibility.\n",
    "#    - 'model.fit(x_train, y_train)' trains the model using the training data (X_train) and the target labels (Y_train).\n",
    "# Outcome: The model is trained using the provided training data and is ready to make predictions or be evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb73d9f-1eb6-4518-868a-ead408efa87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Evaluate the model\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d2281-37ca-4ec6-ad9d-db107b03bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Actions for generating detailed evaluation metrics\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Purpose: The following commands generate and print evaluation metrics to assess the performance of the trained model.\n",
    "# What it does:\n",
    "#    - 'confusion_matrix(y_test, y_pred)' computes the confusion matrix, which shows the number of correct and incorrect predictions broken down by class.\n",
    "#    - 'print(cm)' outputs the confusion matrix for visual inspection.\n",
    "#    - 'classification_report(y_test, y_pred)' generates a report that includes precision, recall, F1-score, and support for each class.\n",
    "# Outcome: These metrics provide detailed insights into the model’s performance, helping you evaluate its effectiveness and identify areas for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a71d13-a80e-462b-8403-5eeed11b1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9: Actions for saving the trained model\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'user_activity_model.pkl')\n",
    "\n",
    "# Purpose: The following commands save the trained machine learning model to a file for later use.\n",
    "# What it does:\n",
    "#    - 'joblib.dump(model, 'user_activity_model.pkl')' saves the trained model (RandomForestClassifier) as a `.pkl` file.\n",
    "#    - The model is saved to the current working directory with the name 'user_activity_model.pkl'.\n",
    "# Outcome: This code enables model persistence, allowing you to reload the model later without retraining it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a61fe7-fc2d-4b45-80fa-ab3c5f991e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10: Actions for loading the trained model and making predictions\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('user_activity_model.pkl')\n",
    "\n",
    "# Predict activity type for a new user\n",
    "new_user_features = pd.DataFrame([[65.0, 2, 1, 0, 0]], columns=[\"avg_session_duration\", \"session_count\", \"activity_type_browsing\", \"activity_type_cart\", \"activity_type_purchase\"])\n",
    "prediction = loaded_model.predict(new_user_features)\n",
    "\n",
    "print(f\"Predicted Activity Type: {prediction}\")\n",
    "\n",
    "# Purpose: The following commands load a previously saved model and use it to predict the activity type for a new user.\n",
    "# What it does:\n",
    "#    - 'joblib.load('user_activity_model.pkl')' loads the saved model from the specified file ('user_activity_model.pkl').\n",
    "#    - A new user’s feature data is created as a pandas DataFrame, with values representing their activity and session characteristics.\n",
    "#    - 'loaded_model.predict(new_user_features)' uses the loaded model to predict the activity type for the new user based on the provided features.\n",
    "# Outcome: This code enables the prediction of activity types for new users by utilizing the trained and saved model.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
